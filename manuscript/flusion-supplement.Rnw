\documentclass{article}

\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

\usepackage{amsmath, amsfonts, amssymb}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{hyperref}

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator{\short}{sh}
\DeclareMathOperator{\Ex}{\mathbb{E}}


\usepackage{setspace}
\onehalfspacing

\usepackage{parskip}

\usepackage{soul}
\usepackage{xcolor}
\def\elr#1{{\color{cyan}\textbf{ELR:[#1]}}}
\def\apg#1{{\color{red}\textbf{APG:[#1]}}}
\def\bwr#1{{\color{violet}\textbf{BWR:[#1]}}}
\def\ngr#1{{\color{blue}\textbf{NGR:[#1]}}}

\usepackage{natbib}
\bibliographystyle{unsrtnat-abbr}


\title{Flusion: Integrating multiple data sources for accurate influenza predictions \\
       \large Supplemental materials}
\author{Evan L. Ray, Yijin Wang, Russ Wolfinger, Nicholas G. Reich}

\begin{document}

\maketitle

<<setup, tidy=FALSE, echo=FALSE, message=FALSE>>=
knitr::opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE)
@

<<environment-setup>>=
library(readr)
library(dplyr)
library(xtable)

source("../code/utils.R")

## necessary to have project root as wd, to override document directory
# knitr::opts_knit$set(root.dir = '../')
@

\section{Introduction}

This document has supplemental materials.

\section{Features}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{../artifacts/figures/features.pdf}
    \caption{Caption about features.}
    \label{fig:forecasts_flusight}
\end{figure}

\section{Feature importance}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{../artifacts/figures/feature_importance.pdf}
    \caption{Caption about feature importance.}
    \label{fig:forecasts_flusight}
\end{figure}

\section{FluSight -- scores omitting forecasts impacted by substantial data revisions}

<<overall-scores, results='asis'>>=
overall_scores_table <- read_csv("../artifacts/scores/scores_by_model_flusight_all.csv") |>
  add_model_anon(
    highlighted_models = c(
      "UMass-flusion" = "Flusion",
      "FluSight-ensemble" = "FluSight-ensemble",
      "FluSight-baseline" = "Baseline-flat",
      "UMass-trends_ensemble" = "Baseline-trend"),
    number_others = TRUE
  ) |>
  mutate(Model = model_anon,
         `\\% Submitted` = prop * 100,
         MWIS = wis,
         RWIS = wis_scaled_relative_skill,
         MAE = ae_median,
         RAE = ae_median_scaled_relative_skill,
         `50\\% PI Cov.` = interval_coverage_50,
         `95\\% PI Cov.` = interval_coverage_95,
         .keep = "none") |>
  arrange(RWIS) |>
  xtable::xtable(
    caption = paste("Overall evaluation results for forecasts submitted to the FluSight Forecast Hub. Model names other than \\textbf{Flusion}, \\textbf{FluSight-ensemble}, \\textbf{Baseline-flat}, and \\textbf{Baseline-trend} are anonymized. The percent of all combinations of location, reference date, and horizon for which the given model submitted forecasts is shown in the ``\\% Submitted\" column; only models submitting at least 75\\% of forecasts were included. Results for the model with the best MWIS, RWIS, MAE, and RAE are highlighted. Results for the models where empirical PI coverage rates are closest to the nominal levels are highlighted."),
    label = "tab:scores_flusight")

digits(overall_scores_table) <- c(0, 0, 1, 1, 3, 1, 3, 3, 3)

# print with specified entries in bold font
# adapted from https://gist.github.com/floybix/452201
bold_entries <- cbind(
  !grepl("Other Model", overall_scores_table$Model),
  rep(FALSE, nrow(overall_scores_table)),
  overall_scores_table$MWIS == min(overall_scores_table$MWIS),
  overall_scores_table$RWIS == min(overall_scores_table$RWIS),
  overall_scores_table$MAE == min(overall_scores_table$MAE),
  overall_scores_table$RAE == min(overall_scores_table$RAE),
  abs(overall_scores_table[["50\\% PI Cov."]] - 0.5) == min(abs(overall_scores_table[["50\\% PI Cov."]] - 0.5)),
  abs(overall_scores_table[["95\\% PI Cov."]] - 0.95) == min(abs(overall_scores_table[["95\\% PI Cov."]] - 0.95))
)

display <- display(overall_scores_table)
digits <- digits(overall_scores_table)
for (i in 1:ncol(overall_scores_table)) {
  if (is.numeric(overall_scores_table[,i])) {
    overall_scores_table[,i] <- formatC(
      overall_scores_table[,i],
      digits = digits[i+1],
      format = display[i+1])
    display(overall_scores_table)[i+1] <- "s"
  }

  ## embolden
  yes <- bold_entries[,i]
  overall_scores_table[yes,i] <- paste("\\textbf{", overall_scores_table[yes,i], "}", sep = "")
}

print(overall_scores_table,
      include.rownames = FALSE, sanitize.text.function = function(x) {x})
@

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{../artifacts/figures/scores_flusight.pdf}
    \caption{Influenza data and evaluation results. Panel (a): Target influenza hospital admissions data for the 2023/24 season, aggregated across all forecasted state-level geographic units. Panel (b): RWIS for models contributing to the FluSight Forecast Hub, stratified by forecast horizon (panels) and target date (horizontal axis). Lower relative WIS indicates better forecast performance. To focus on areas of interest, RWIS values greater than 2.5 are not displayed. Panel (c): One-sided quantile coverage differential, computed as empirical coverage rate minus nominal coverage rate. A well-calibrated model has a differential of 0, while a conservative method (with wide prediction intervals) has a negative differential at nominal coverage rates that are less than 0.5 and a positive differential at nominal coverage rates greater than 0.5. In panels (b) and (c), we highlight performance for the Flusion model, two baselines, and the FluSight ensemble; results for other models contributed to the hub are shown in light grey.}
    \label{fig:scores_flusight}
\end{figure}

\bibliography{flusion}

\end{document}
