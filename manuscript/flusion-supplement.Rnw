\documentclass{article}

\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

\usepackage{amsmath, amsfonts, amssymb}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{hyperref}

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator{\short}{sh}
\DeclareMathOperator{\Ex}{\mathbb{E}}


\usepackage{setspace}
\onehalfspacing

\usepackage{parskip}
\usepackage{authblk}

\usepackage{soul}
\usepackage{xcolor}
\def\elr#1{{\color{cyan}\textbf{ELR:[#1]}}}
\def\apg#1{{\color{red}\textbf{APG:[#1]}}}
\def\bwr#1{{\color{violet}\textbf{BWR:[#1]}}}
\def\ngr#1{{\color{blue}\textbf{NGR:[#1]}}}

%\usepackage{natbib}
%\bibliographystyle{unsrtnat-abbr}
\usepackage{biblatex} %Imports biblatex package
\addbibresource{flusion.bib} %Import the bibliography file


\title{Flusion: Integrating multiple data sources for accurate influenza predictions \\
       Supplemental materials}
\author[1]{Evan L. Ray}
\author[1]{Yijin Wang}
\author[2]{Russell D. Wolfinger}
\author[1]{Nicholas G. Reich}
\affil[1]{Department of Biostatistics and Epidemiology, University of Massachusetts, Amherst, MA, United States}
\affil[2]{SAS Institute Inc., Cary, NC, United States}

\begin{document}

\maketitle

<<setup, tidy=FALSE, echo=FALSE, message=FALSE>>=
knitr::opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE)
@

<<environment-setup>>=
library(readr)
library(dplyr)
library(xtable)

source("../code/utils.R")

## necessary to have project root as wd, to override document directory
# knitr::opts_knit$set(root.dir = '../')
@

\section{Introduction}

This document provides supplemental analyses and results for the Flusion manuscript. Section \ref{sec:flusurv_burden_adj} describes the approach we use to adjusting the FluSurv-NET data to account for testing rates and test sensitivity in that surveillance system.  Section \ref{sec:feats} describes the features used in the GBQR model that summarize local behavior of the target surveillance signal.  Section \ref{sec:feature_importance} presents results about the importance of the features used by the GBQR model. Sections \ref{sec:flusight_sensitivity} and \ref{sec:experiments_sensitivity} describe sensitivity analyses investigating whether the forecast evaluation results in the main text are impacted by data revisions.

\section{Reporting adjustments for FluSurv-NET data}
\label{sec:flusurv_burden_adj}

In this section, we describe the adjustments we made to the reported FluSurv-NET data. The measure of influenza activity reported by FluSurv-NET is the rate of positive influenza cases per 100,000 population in the catchment area of a reporting healthcare facility or group of facilities. For the purposes of FluSurv-NET, ``a case is defined as a person who is a resident in a defined FluSurv-NET catchment area and tests positive for influenza by a laboratory test ordered by a health care professional within 14 days prior to or during hospitalization'' \cite{cdc_flusurvnet}. This measure of influenza activity may be impacted by underdetection of influenza cases either if patients with influenza are not tested or if they are tested but the test generates a false negative result.

The US Centers for Disease Control and Prevention (CDC) produces annual estimates of influenza disease burden at the national level that adjust for testing rates and test sensitivity, including point estimates of total nationwide influenza hospitalizations in each season along with 95\% uncertainty intervals \cite{cdc_flu_burden_methods, cdc_flu_burden}. We used these to estimate a season-specific scale up factor $\alpha$ that was used to adjust the FluSurv-NET data. This factor was obtained by solving the following equation for $\alpha$ based on the total hospitalization rate over the course of the season that was reported across the entire FluSurv-NET network, the point estimate of national hospital burden due to influenza from CDC, and the US population in units of 100,000 people as reported by the US Census Bureau for the first year of the influenza season \cite{census_pop_older, census_pop_recent}:
$$\alpha \cdot (\text{cumulative reported hospitalization rate, FluSurv-NET})
= \frac{\text{National burden estimate}}{\text{100k US population}}$$

Table \ref{tab:flusurv_burden_adj} summarizes these terms and the resulting estimated scale-up factors for each season with FluSurv-NET data in our training set. Note that the scale-up factors are larger in earlier seasons than later seasons, indicating that data from FluSurv-NET undercounted influenza activity more in earlier seasons.

<<burden-adj, results='asis'>>=
burden_adj <- read_csv("../artifacts/flusurv_burden_adj.csv") |>
  dplyr::mutate(
    `Season` = season,
    `Cum. rate` = cum_rate,
    `US population` = formatC(pop, big.mark = ",", digits=10),
    `Est. burden (count)` = formatC(hosp_burden, big.mark = ",", digits=8),
    `Est. burden (rate)` = hosp_burden / (pop / 100000),
    `$\\alpha$` = adj_factor,
    .keep = "none"
  )

table <- xtable::xtable(
  burden_adj,
  caption = "Reported data, intermediate calculations, and final estimates for FluSurv-NET burden adjustments in each training season where we used FluSurv-NET data. The 'Cum. rate' column shows the cumulative reported hospitalization rate over the course of the season for the entire FluSurv-NET network. The US populaton column shows an estimate of the US population size from the US Census Bureau in the first year of the season (e.g., the value shown for the 2010/11 season is the population estimate for 2010). The 'Est. burden (count)' column shows the point estimate of influenza hospitalization burden produced by CDC for each season, and the `Est. burden (rate)` column expresses these burden estimates as a rate per 100,000 population in the US by dividing the estimated burden count by the US population in units of 100,000 people. The scale-up factor $\\alpha$ is the ratio of the values in the 'Est. burden (rate)' and 'Cum. rate' columns.",
  label = "tab:flusurv_burden_adj")

align(table) <- rep("r", 7)
digits(table) <- c(0, 0, 1, 0, 0, 1, 1)

print(
  table,
  include.rownames = FALSE,
  sanitize.text.function = function(x) {x})
@

\section{Features measuring local level, slope, and curvature of the surveillance signal}
\label{sec:feats}

As was described in section 5 of the main text, the GBQR models used features based on rolling means and the coefficients of Taylor polynomials fit to rolling windows of the data. These features are designed to estimate the local level, slope, and curvature of the surveillance signal at each point in time, and we describe their calculation here. Recall the notation $\tilde{z}_{l,s,t}$ representing the value of the signal for location $l$ and data source $s$ at time $t$, after some initial standardizing transformations as described in section 5.1 of the main text.

At time $t$, the rolling mean over the trailing window of length $w$ is computed as
\begin{equation}
\frac{1}{w} \sum_{u = t - w + 1}^t \tilde{z}_{l,s,u}. \label{eqn:roll_mean}
\end{equation}

The coefficients of a degree $d$ Taylor polynomial based on the trailing window of length $w$ relative to the anchor point $t$ are obtained by fitting the following model to the observations $\{\tilde{z}_{l,s,u}: u = t - w + 1, \ldots, t\}$:
\begin{align}
\tilde{z}_{l,s,u} &= \sum_{c = 0}^d \frac{1}{c!}\beta_c (u - t)^c + \varepsilon_u \label{eqn:taylor_model} \\
\varepsilon_u &\sim \text{Normal}(0, \sigma^2) \nonumber
\end{align}
For example, with $d = 2$ we fit the quadratic model
\begin{align*}
\tilde{z}_{l,s,u} &= \beta_0 + \beta_1 (u - t) + \frac{1}{2}\beta_2 (u - t)^2 + \varepsilon_u \\
\varepsilon_u &\sim \text{Normal}(0, \sigma^2)
\end{align*}
To motivate this, suppose that the underlying signal follows a mean trend over time given by the smooth function $g(u)$, with observation noise due to, e.g., the reporting process. The function $g$ can be written in terms of its derivatives $g^{(c)}$ using the Taylor expansion about the point $t$:
$$
g(u) = \sum_{c = 0}^\infty \frac{g^{(c)}(t)}{c!} (u - t)^c.
$$
Truncating to the first $d+1$ terms yields an approximation to $g$ in the neighborhood of $t$, and the coefficient estimates $\beta_c$ from the linear model \eqref{eqn:taylor_model} can be regarded as estimates of the corresponding derivatives $g^{(c)}(t)$.
We refer to estimates of $\beta_0$, $\beta_1$, and $\beta_2$ as estimates of the local level, trend, and curvature of the signal respectively. The highest degree we used in any of our feature computations was $d = 2$.
Note that the rolling mean of Equation \eqref{eqn:roll_mean} could also be obtained from this process using a Taylor polynomial of degree $d = 0$, though in practice we used a more direct implementation.

Figure \ref{fig:features} illustrates the values of these features for the NHSN admission signal in the state of Michigan in the 2023/24 season. As expected, the features calculated based on longer window sizes $w$ and lower polynomial degrees $d$ vary more smoothly over time than features calculated based on shorter windows or higher polynomial degrees. Nevertheless, the features generally agree in terms of when the slope and curvature are positive or negative.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{../artifacts/figures/features.pdf}
    \caption{Example of features measuring the local level, trend, and curvature of the standardized NHSN admissions signal for the state of Michigan in the 2023/24 season (shown in black in the top panel for reference). At each time on the horizontal axis, a vertical line will intersect features calculated based on a trailing window ending on that date. For example, on Christmas week (just before Jan 2024), features based on a Taylor polynomial of degree $d=2$ fit to a trailing window of size $w = 4$ produced a local level estimate that closely matched the Christmas peak observed in the data, a positive slope just over 0.1 on the scale of the standardized data, and a positive curvature just under 0.05 indicating that the trend was increasing over that four week period.}
    \label{fig:features}
\end{figure}

Note that at the end of the signal, only observations on or before the last time point are available. This motivates the use of a trailing window for feature calculation: with this choice, the features computed at both the end of the time series and at earlier time points can be expected to have similar characteristics as measures of local derivatives of the signal's trend. In contrast, if a centered window were used, estimates at earlier time points (when all observations within the centered window are available) would be more reliable than estimates at the end of the series.

Importantly, we do not account for the history of data revisions when we calculate these features. For example, for model fitting on reference date $t$, training examples are assembled for past times $u < t$ that include features measuring the local level, slope, and curvature at those times $u$. Those features are calculated based on the latest available data at time $t$, not based on the data that would have been available at time $u$. This means that our model implicitly estimates the relationships between these features and the target when the features are calculated on finalized, fully reported data. However, when predictions are generated extending from the reference date $t$, those features are calculated at the end of the time series when reported values more likely to be subsequently revised, leading to a mismatch between the data used for model fitting and the data used for prediction. This is a challenging problem to address in a setting like ours where the target data system has only a short reporting history and the characteristics of its revision process are not well known.

Finally, we highlight that although features such as the rolling mean or the intercept of a Taylor polynomial only directly measure the local level of the signal, when their lags are also included as features they can provide information about trend as well. For example, if we see that the rolling mean at time $t$ is larger than the rolling mean at time $t-1$ we may infer that the value of the signal is rising.

\section{Feature importance}
\label{sec:feature_importance}

Figure \ref{fig:feature_importance} shows feature importance scores for the GBQR fit for the reference date of January 6, 2024. See section 7 of the main text for more detail on how importance scores were calculated.

\begin{figure}[htp]
    \centering
    \includegraphics[width=\textwidth]{../artifacts/figures/feature_importance.pdf}
    \caption{Feature importance values for the GBQR fit for the reference date of January 6, 2024. ``Signal value" indicates the latest reported value of the signal. For Taylor polynomial features, ``d'' indicates the polynomial degree and ``c'' indicates the coefficient corresponding to the feature value. For Taylor polynomial and rolling mean features, ``w'' indicates the window size used for feature calculation and if present ``lag" indicates the weekly lag used for the feature value.}
    \label{fig:feature_importance}
\end{figure}

\section{FluSight results: sensitivity analysis for data revisions}
\label{sec:flusight_sensitivity}

<<revisions-dropped-info>>=
source("../code/scoring_helpers.R")

# load data
ref_date_loc_revised <- get_ref_date_loc_revised(
  rel_path_prefix = "..",
  min_revision_size = 10)
ref_date_loc_all <- get_ref_date_loc_revised(
  rel_path_prefix = "..",
  min_revision_size = 0)
@

Table \ref{tab:scores_flusight} contains MAE, MWIS, and PI coverage rates for real-time FluSight predictions, omitting predictions made on combinations of location and reference date for which the most recent available data at the time the prediction was generated were subsequently revised by 10 or more admissions.  This represents a generous sensitivity analysis, omitting \Sexpr{nrow(ref_date_loc_revised)} out of \Sexpr{nrow(ref_date_loc_all)} combinations of location and reference date for which predictions were submitted.  Figure \ref{fig:revisions-dropped-size-plot} displays information about the magnitudes of these revisions.

Comparing with Table 1 in the primary manuscript, we note that the main results discussed there still hold: Flusion has the best MAE and MWIS values by a substantial margin, while the marginal coverage rates of its central prediction intervals are too conservative.

<<overall-scores, results='asis'>>=
overall_scores <- read_csv("../artifacts/scores/scores_by_model_flusight_all_without_revisions.csv") |>
  add_model_anon(
    highlighted_models = c(
      "UMass-flusion" = "Flusion",
      "FluSight-ensemble" = "FluSight-ensemble",
      "FluSight-baseline" = "Baseline-flat",
      "UMass-trends_ensemble" = "Baseline-trend"),
    number_others = TRUE
  ) |>
  mutate(Model = model_anon)

make_scores_table <- function(scores, caption, label, bold_modelnames = TRUE) {
  scores_table <- scores |>
    mutate(Model = Model,
           `\\% Submitted` = prop * 100,
           MWIS = wis,
           RWIS = wis_scaled_relative_skill,
           MAE = ae_median,
           RAE = ae_median_scaled_relative_skill,
           `50\\% Cov.` = interval_coverage_50,
           `95\\% Cov.` = interval_coverage_95,
           .keep = "none") |>
    arrange(RWIS) |>
    xtable::xtable(
      caption = caption,
      label = label)

  digits(scores_table) <- c(0, 0, 1, 1, 3, 1, 3, 3, 3)

  # print with specified entries in bold font
  # adapted from https://gist.github.com/floybix/452201
  if (bold_modelnames) {
    bold_modelnames <- !grepl("Other Model", scores_table$Model)
  } else {
    bold_modelnames <- rep(FALSE, nrow(scores_table))
  }
  bold_entries <- cbind(
    bold_modelnames,
    rep(FALSE, nrow(scores_table)),
    scores_table$MWIS == min(scores_table$MWIS),
    scores_table$RWIS == min(scores_table$RWIS),
    scores_table$MAE == min(scores_table$MAE),
    scores_table$RAE == min(scores_table$RAE),
    abs(scores_table[["50\\% Cov."]] - 0.5) == min(abs(scores_table[["50\\% Cov."]] - 0.5)),
    abs(scores_table[["95\\% Cov."]] - 0.95) == min(abs(scores_table[["95\\% Cov."]] - 0.95))
  )

  display <- display(scores_table)
  digits <- digits(scores_table)
  for (i in 1:ncol(scores_table)) {
    if (is.numeric(scores_table[,i])) {
      scores_table[,i] <- formatC(
        scores_table[,i],
        digits = digits[i+1],
        format = display[i+1])
      display(scores_table)[i+1] <- "s"
    }

    ## embolden
    yes <- bold_entries[,i]
    scores_table[yes,i] <- paste("\\textbf{", scores_table[yes,i], "}", sep = "")
  }
  
  return(scores_table)
}

overall_scores_table_revisions <- make_scores_table(
  overall_scores,
  caption = paste("Overall evaluation results for forecasts submitted to the FluSight Forecast Hub, omitting forecasts made on combinations of reference date and location for which the latest available NHSN data at the time of the forecast were subsequently revised by 10 or more admissions. Model names other than Flusion, FluSight-ensemble, Baseline-flat, and Baseline-trend are anonymized. The percent of all combinations of location, reference date, and horizon for which the given model submitted forecasts is shown in the ``\\% Submitted\" column; only models submitting at least 2/3 of forecasts were included. Results for the model with the best MWIS, RWIS, MAE, and RAE are highlighted. Results for the models where empirical PI coverage rates are closest to the nominal levels are highlighted."),
  label = "tab:scores_flusight"
)

print(overall_scores_table_revisions,
      include.rownames = FALSE, sanitize.text.function = function(x) {x})
@


<<revisions-dropped-size-plot, fig.height=6, fig.width=7.5, fig.cap="Measures of the size of reporting revisions for combinations of location and reference date that were omitted in the sensitivity analysis.  For legibility, only those revisions that were dropped (i.e., where the revision amount was at least 10 admissions up or down from the initial reported value) are displayed; most revisions were small.  The top panel shows the size of the revision in units of hospital admissions, where positive numbers indicate an upward revision of the initially reported value.  The second panel shows the absolute value of the revision size as a proportion of the final reported value.  The third panel shows the absolute value of the revision size as a proportion of the initial reported value.  When computing proportions, we add one to the denominator to avoid division by zero.  As an example, for October 7, 2023 (the last date for which data were available when producing predictions with a reference date of October 14, 2023), in Washington state the initial reported value was 43, which was subsequently revised down to a final value of 4. The revision amount is -39, which is 7.80 when expressed as a proportion of the final reported value or 0.89 when expressed as a proportion of the initial reported value.">>=
library(ggplot2)
library(tidyr)

ref_date_loc_revised |>
  mutate(revision_size = value - initial_value) |>
  pivot_longer(starts_with("revision_"), values_to = "revision_value") |>
  mutate(
    name = case_when(
      name == "revision_size" ~ "Number of admissions",
      name == "revision_prop" ~ "Proportion of final value",
      name == "revision_prop2" ~ "Proportion of initial value"
    )
  ) |>
  ggplot() +
    geom_histogram(mapping = aes(x = revision_value), boundary=0) +
    facet_wrap( ~ name, ncol = 1, scales = "free_x") +
    xlab("Revision size") + 
    theme_bw()
@

\section{Experimental results: sensitivity analysis for data revisions}
\label{sec:experiments_sensitivity}

Table \ref{tab:scores_experiments} contains results from the post hoc experiments described in section 7 of the main text, omitting forecasts produced for combinations of location and reference date where the latest available NHSN data as of the reference date were subsequently revised up or down by at least 10 admissions. Comparing with table 2 of the main text, we see that the qualitative modeling results discussed there still hold in this sensitivity analysis.

<<posthoc-scores, results='asis'>>=
component_scores <- read_csv("../artifacts/scores/scores_by_model_flusion_components_without_revisions.csv") |>
  mutate(
    Model = case_when(
      model == "UMass-flusion__gbq_qr__sarix" ~ "GBQR, ARX",
      model == "UMass-flusion" ~ "Flusion",
      model == "UMass-gbq_qr" ~ "GBQR",
      model == "UMass-flusion__gbq_qr__gbq_qr_no_level" ~ "GBQR, GBQR-no-level",
      model == "UMass-flusion__gbq_qr_no_level__sarix" ~ "GBQR-no-level, ARX",
      model == "UMass-gbq_qr_no_level" ~ "GBQR-no-level",
      model == "UMass-sarix" ~ "ARX",
      model == "FluSight-baseline" ~ "Baseline-flat"
    )
  )

component_scores_table <- make_scores_table(
  component_scores,
  caption = NULL,
  label = "junk label",
  bold_modelnames = FALSE
)
table_2a <- capture.output(
  print(component_scores_table,
        include.rownames = FALSE, sanitize.text.function = function(x) {x})
)
table_2a <- c(
  table_2a[1:5],
  "\\multicolumn{8}{l}{Experiment A: Component model performance} \\\\",
  table_2a[6:17]
)

joint_training_scores <- read_csv("../artifacts/scores/scores_by_model_joint_training_without_revisions.csv") |>
  mutate(
    Model = case_when(
      model == "UMass-gbq_qr" ~ "GBQR",
      model == "UMass-gbq_qr_fit_locations_separately" ~ "GBQR-by-location",
      model == "UMass-gbq_qr_hhs_only" ~ "GBQR-only-NHSN",
      model == "FluSight-baseline" ~ "Baseline-flat"
    )
  )

joint_training_scores_table <- make_scores_table(
  joint_training_scores,
  caption = NULL,
  label = "junk label",
  bold_modelnames = FALSE
)
table_2b <- capture.output(
  print(joint_training_scores_table,
        include.rownames = FALSE, sanitize.text.function = function(x) {x})
)
table_2b <- c(
  "\\\\",
  "\\multicolumn{8}{l}{Experiment B: Reduced training data} \\\\",
  table_2b[6:13]
)

preprocessing_scores <- read_csv("../artifacts/scores/scores_by_model_flusion_data_adj_without_revisions.csv") |>
  mutate(
    Model = case_when(
      model == "UMass-gbq_qr" ~ "GBQR",
      model == "UMass-gbq_qr_no_reporting_adj" ~ "GBQR-no-reporting-adj",
      model == "UMass-gbq_qr_no_transform" ~ "GBQR-no-transform",
      model == "FluSight-baseline" ~ "Baseline-flat"
    )
  )

preprocessing_scores_table <- make_scores_table(
  preprocessing_scores,
  caption = paste("Evaluation results for post hoc experiments investigating determinants of model performance, omitting forecasts made on combinations of reference date and location for which the latest available NHSN data at the time of the forecast were subsequently revised by 10 or more admissions.  Experiment A gives results for individual component models in the Flusion ensemble, ensembles of pairs of components, and the full Flusion ensemble including all three components.  Experiment B gives results for the GBQR model, which is trained jointly on data for all locations and data sources, and variations trained separately for each location (GBQR-by-location) and trained only on hospital admissions from NHSN (GBQR-only-NHSN).  Experiment C gives results for a variation on the GBQR model that does not incorporate reporting adjustments designed to improve the degree to which ILINet and FluSurvNET data reflect influenza activity (GBQR-no-reporting-adj) and a variation that does not use a fourth-root transform (GBQR-no-transform), along with the original GBQR model which uses the reporting adjustments and the fourth-root transform. The percent of all combinations of location, reference date, and horizon for which the given model submitted forecasts is shown in the ``\\% Submitted\" column; in these retrospective experiments, we produced forecasts for all locations and time points. Within each experiment group, results for the model with the best MWIS, RWIS, MAE, and RAE are highlighted. Results for the models where empirical PI coverage rates are closest to the nominal levels are highlighted."),
  label = "tab:scores_experiments",
  bold_modelnames = FALSE
)
table_2c <- capture.output(
  print(preprocessing_scores_table,
        include.rownames = FALSE, sanitize.text.function = function(x) {x})
)

table_2c <- c(
  "\\\\",
  "\\multicolumn{8}{l}{Experiment C: Data preprocessing} \\\\",
  table_2c[6:17]
)

table_2 <- c(table_2a, table_2b, table_2c)
cat(table_2, sep = "\n")
@

\printbibliography

\end{document}
