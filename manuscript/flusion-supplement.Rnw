\documentclass{article}

\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

\usepackage{amsmath, amsfonts, amssymb}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{hyperref}

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator{\short}{sh}
\DeclareMathOperator{\Ex}{\mathbb{E}}


\usepackage{setspace}
\onehalfspacing

\usepackage{parskip}

\usepackage{soul}
\usepackage{xcolor}
\def\elr#1{{\color{cyan}\textbf{ELR:[#1]}}}
\def\apg#1{{\color{red}\textbf{APG:[#1]}}}
\def\bwr#1{{\color{violet}\textbf{BWR:[#1]}}}
\def\ngr#1{{\color{blue}\textbf{NGR:[#1]}}}

\usepackage{natbib}
\bibliographystyle{unsrtnat-abbr}


\title{Flusion: Integrating multiple data sources for accurate influenza predictions \\
       Supplemental materials}
\author{Evan L. Ray, Yijin Wang, Russ Wolfinger, Nicholas G. Reich}

\begin{document}

\maketitle

<<setup, tidy=FALSE, echo=FALSE, message=FALSE>>=
knitr::opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE)
@

<<environment-setup>>=
library(readr)
library(dplyr)
library(xtable)

source("../code/utils.R")

## necessary to have project root as wd, to override document directory
# knitr::opts_knit$set(root.dir = '../')
@

\section{Introduction}

This document has supplemental materials.

\section{Reporting adjustments for FluSurv-NET data}




\section{Features measuring local level, slope, and curvature of the surveillance signal}

As was described in section 5 of the main text, the GBQR models used features based on rolling means and the coefficients of Taylor polynomials fit to rolling windows of the data. These features are designed to estimate the local level, slope, and curvature of the surveillance signal at each point in time, and we describe their calculation here. Recall the notation $\tilde{z}_{l,s,t}$ representing the value of the signal for location $l$ and data source $s$ at time $t$, after some initial standardizing transformations as described in section 5.1 of the main text.

At time $t$, the rolling mean over the trailing window of length $w$ is computed as
\begin{equation}
\frac{1}{w} \sum_{u = t - w + 1}^t \tilde{z}_{l,s,u}. \label{eqn:roll_mean}
\end{equation}

The coefficients of a degree $d$ Taylor polynomial based on the trailing window of length $w$ relative to the anchor point $t$ are obtained by fitting the following model to the observations $\{\tilde{z}_{l,s,u}: u = t - w + 1, \ldots, t\}$:
\begin{align}
\tilde{z}_{l,s,u} &= \sum_{c = 0}^d \frac{1}{c!}\beta_c (u - t)^c + \varepsilon_u \label{eqn:taylor_model} \\
\varepsilon_u &\sim \text{Normal}(0, \sigma^2) \nonumber
\end{align}
For example, with $d = 2$ we fit the quadratic model
\begin{align*}
\tilde{z}_{l,s,u} &= \beta_0 + \beta_1 (u - t) + \frac{1}{2}\beta_2 (u - t)^2 + \varepsilon_u \\
\varepsilon_u &\sim \text{Normal}(0, \sigma^2)
\end{align*}
To motivate this, suppose that the underlying signal follows a mean trend over time given by the smooth function $g(u)$, with observation noise due to, e.g., the reporting process. The function $g$ can be written in terms of its derivatives $g^{(c)}$ using the Taylor expansion about the point $t$:
$$
g(u) = \sum_{c = 0}^\infty \frac{g^{(c)}(t)}{c!} (u - t)^c.
$$
Truncating to the first $d+1$ terms yields an approximation to $g$ in the neighborhood of $t$, and the coefficient estimates $\beta_c$ from the linear model \eqref{eqn:taylor_model} can be regarded as estimates of the corresponding derivatives $g^{(c)}(t)$.
We refer to estimates of $\beta_0$, $\beta_1$, and $\beta_2$ as estimates of the local level, trend, and curvature of the signal respectively. The highest degree we used in any of our feature computations was $d = 2$.
Note that the rolling mean of Equation \eqref{eqn:roll_mean} could also be obtained from this process using a Taylor polynomial of degree $d = 0$, though in practice we used a more direct implementation.

Figure \ref{fig:forecasts_flusight} illustrates the values of these features for the NHSN admission signal in the state of Michigan in the 2023/24 season. As expected, the features calculated based on longer window sizes $w$ and lower polynomial degrees $d$ vary more smoothly over time than features calculated based on shorter windows or higher polynomial degrees. Nevertheless, the features generally agree in terms of when the slope and curvature are positive or negative.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{../artifacts/figures/features.pdf}
    \caption{Example of features measuring the local level, trend, and curvature of the standardized NHSN admissions signal for the state of Michigan in the 2023/24 season (shown in black in the top panel for reference). At each time on the horizontal axis, a vertical line will intersect features calculated based on a trailing window ending on that date. For example, on Christmas week (just before Jan 2024), features based on a Taylor polynomial of degree $d=2$ fit to a trailing window of size $w = 4$ produced a local level estimate that closely matched the Christmas peak observed in the data, a positive slope just over 0.1 on the scale of the standardized data, and a positive curvature just under 0.05 indicating that the trend was increasing over that four week period.}
    \label{fig:forecasts_flusight}
\end{figure}

Note that at the end of the signal, only observations on or before the last time point are available. This motivates the use of a trailing window for feature calculation: with this choice, the features computed at both the end of the time series and at earlier time points can be expected to have similar characteristics as measures of local derivatives of the signal's trend. In contrast, if a centered window were used, estimates at earlier time points (when all observations within the centered window are available) would be more reliable than estimates at the end of the series.

Importantly, we do not account for the history of data revisions when we calculate these features. For example, for model fitting on reference date $t$, training examples are assembled for past times $u < t$ that include features measuring the local level, slope, and curvature at those times $u$. Those features are calculated based on the latest available data at time $t$, not based on the data that would have been available at time $u$. This means that our model implicitly estimates the relationships between these features and the target when the features are calculated on finalized, fully reported data. However, when predictions are generated extending from the reference date $t$, those features are calculated at the end of the time series when reporting is more likely to be subsequently revised, leading to a mismatch between the data used for model fitting and the data used for prediction. This is a challenging problem to address in a setting like ours where the target data system has only a short reporting history and the characteristics of its revision process are not well known.

Finally, we highlight that although features such as the rolling mean or the intercept of a Taylor polynomial only directly measure the local level of the signal, when their lags are also included as features they can provide information about trend as well. For example, if we see that the rolling mean at time $t$ is larger than the rolling mean at time $t-1$ we may infer that the value of the signal is rising.

\section{Feature importance}

TODO clean up names of features in figure, describe how importance is calculated

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{../artifacts/figures/feature_importance.pdf}
    \caption{Caption about feature importance.}
    \label{fig:forecasts_flusight}
\end{figure}

\section{FluSight results: sensitivity analysis for data revisions}

<<revisions-dropped-info>>=
source("../code/scoring_helpers.R")

# load data
ref_date_loc_revised <- get_ref_date_loc_revised(
  rel_path_prefix = "..",
  min_revision_size = 10)
ref_date_loc_all <- get_ref_date_loc_revised(
  rel_path_prefix = "..",
  min_revision_size = 0)
@

Table \ref{tab:scores_flusight} contains MAE, MWIS, and PI coverage rates for real-time FluSight predictions, omitting predictions made on combinations of location and reference date for which the most recent available data at the time the prediction was generated were subsequently revised by 10 or more admissions.  This represents a generous sensitivity analysis, omitting \Sexpr{nrow(ref_date_loc_revised)} out of \Sexpr{nrow(ref_date_loc_all)} combinations of location and reference date for which predictions were submitted.  Figure \ref{fig:revisions-dropped-size-plot} displays information about the magnitudes of these revisions.

Comparing with Table 1 in the primary manuscript, we note that the main results discussed there still hold: Flusion has the best MAE and MWIS values by a substantial margin, while the marginal coverage rates of its central prediction intervals are too conservative.

<<overall-scores, results='asis'>>=
overall_scores <- read_csv("../artifacts/scores/scores_by_model_flusight_all_without_revisions.csv") |>
  add_model_anon(
    highlighted_models = c(
      "UMass-flusion" = "Flusion",
      "FluSight-ensemble" = "FluSight-ensemble",
      "FluSight-baseline" = "Baseline-flat",
      "UMass-trends_ensemble" = "Baseline-trend"),
    number_others = TRUE
  ) |>
  mutate(Model = model_anon)

make_scores_table <- function(scores, caption, label, bold_modelnames = TRUE) {
  scores_table <- scores |>
    mutate(Model = Model,
           `\\% Submitted` = prop * 100,
           MWIS = wis,
           RWIS = wis_scaled_relative_skill,
           MAE = ae_median,
           RAE = ae_median_scaled_relative_skill,
           `50\\% Cov.` = interval_coverage_50,
           `95\\% Cov.` = interval_coverage_95,
           .keep = "none") |>
    arrange(RWIS) |>
    xtable::xtable(
      caption = caption,
      label = label)

  digits(scores_table) <- c(0, 0, 1, 1, 3, 1, 3, 3, 3)

  # print with specified entries in bold font
  # adapted from https://gist.github.com/floybix/452201
  if (bold_modelnames) {
    bold_modelnames <- !grepl("Other Model", scores_table$Model)
  } else {
    bold_modelnames <- rep(FALSE, nrow(scores_table))
  }
  bold_entries <- cbind(
    bold_modelnames,
    rep(FALSE, nrow(scores_table)),
    scores_table$MWIS == min(scores_table$MWIS),
    scores_table$RWIS == min(scores_table$RWIS),
    scores_table$MAE == min(scores_table$MAE),
    scores_table$RAE == min(scores_table$RAE),
    abs(scores_table[["50\\% Cov."]] - 0.5) == min(abs(scores_table[["50\\% Cov."]] - 0.5)),
    abs(scores_table[["95\\% Cov."]] - 0.95) == min(abs(scores_table[["95\\% Cov."]] - 0.95))
  )

  display <- display(scores_table)
  digits <- digits(scores_table)
  for (i in 1:ncol(scores_table)) {
    if (is.numeric(scores_table[,i])) {
      scores_table[,i] <- formatC(
        scores_table[,i],
        digits = digits[i+1],
        format = display[i+1])
      display(scores_table)[i+1] <- "s"
    }

    ## embolden
    yes <- bold_entries[,i]
    scores_table[yes,i] <- paste("\\textbf{", scores_table[yes,i], "}", sep = "")
  }
  
  return(scores_table)
}

overall_scores_table_revisions <- make_scores_table(
  overall_scores,
  caption = paste("Overall evaluation results for forecasts submitted to the FluSight Forecast Hub, omitting forecasts made on combinations of reference date and location for which the latest available NHSN data at the time of the forecast were subsequently revised by 10 or more admissions. Model names other than Flusion, FluSight-ensemble, Baseline-flat, and Baseline-trend are anonymized. The percent of all combinations of location, reference date, and horizon for which the given model submitted forecasts is shown in the ``\\% Submitted\" column; only models submitting at least 2/3 of forecasts were included. Results for the model with the best MWIS, RWIS, MAE, and RAE are highlighted. Results for the models where empirical PI coverage rates are closest to the nominal levels are highlighted."),
  label = "tab:scores_flusight"
)

print(overall_scores_table_revisions,
      include.rownames = FALSE, sanitize.text.function = function(x) {x})
@


<<revisions-dropped-size-plot, fig.height=6, fig.width=7.5, fig.cap="Measures of the size of reporting revisions for combinations of location and reference date that were omitted in the sensitivity analysis.  For legibility, only those revisions that were dropped (i.e., where the revision amount was at least 10 admissions up or down from the initial reported value) are displayed; most revisions were small.  The top panel shows the size of the revision in units of hospital admissions, where positive numbers indicate an upward revision of the initially reported value.  The second panel shows the absolute value of the revision size as a proportion of the final reported value.  The third panel shows the absolute value of the revision size as a proportion of the initial reported value.  When computing proportions, we add one to the denominator to avoid division by zero.  As an example, for October 7, 2023 (the last date for which data were available when producing predictions with a reference date of October 14, 2023), in Washington state the initial reported value was 43, which was subsequently revised down to a final value of 4. The revision amount is -39, which is 7.80 when expressed as a proportion of the final reported value or 0.89 when expressed as a proportion of the initial reported value.">>=
library(ggplot2)
library(tidyr)

ref_date_loc_revised |>
  mutate(revision_size = value - initial_value) |>
  pivot_longer(starts_with("revision_"), values_to = "revision_value") |>
  mutate(
    name = case_when(
      name == "revision_size" ~ "Number of admissions",
      name == "revision_prop" ~ "Proportion of final value",
      name == "revision_prop2" ~ "Proportion of initial value"
    )
  ) |>
  ggplot() +
    geom_histogram(mapping = aes(x = revision_value), boundary=0) +
    facet_wrap( ~ name, ncol = 1, scales = "free_x") +
    xlab("Revision size") + 
    theme_bw()
@

\section{Experimental results: sensitivity analysis for data revisions}

Table \ref{tab:scores_experiments} contains results from the post hoc experiments described in section 7 of the main text, omitting forecasts produced for combinations of location and reference date where the latest available NHSN data as of the reference date were subsequently revised up or down by at least 10 admissions. Comparing with table 2 of the main text, we see that the qualitative modeling results discussed there still hold in this sensitivity analysis.

<<posthoc-scores, results='asis'>>=
component_scores <- read_csv("../artifacts/scores/scores_by_model_flusion_components_without_revisions.csv") |>
  mutate(
    Model = case_when(
      model == "UMass-flusion__gbq_qr__sarix" ~ "GBQR, ARX",
      model == "UMass-flusion" ~ "Flusion",
      model == "UMass-gbq_qr" ~ "GBQR",
      model == "UMass-flusion__gbq_qr__gbq_qr_no_level" ~ "GBQR, GBQR-no-level",
      model == "UMass-flusion__gbq_qr_no_level__sarix" ~ "GBQR-no-level, ARX",
      model == "UMass-gbq_qr_no_level" ~ "GBQR-no-level",
      model == "UMass-sarix" ~ "ARX",
      model == "FluSight-baseline" ~ "Baseline-flat"
    )
  )

component_scores_table <- make_scores_table(
  component_scores,
  caption = NULL,
  label = "junk label",
  bold_modelnames = FALSE
)
table_2a <- capture.output(
  print(component_scores_table,
        include.rownames = FALSE, sanitize.text.function = function(x) {x})
)
table_2a <- c(
  table_2a[1:5],
  "\\multicolumn{8}{l}{Experiment A: Component model performance} \\\\",
  table_2a[6:17]
)

joint_training_scores <- read_csv("../artifacts/scores/scores_by_model_joint_training_without_revisions.csv") |>
  mutate(
    Model = case_when(
      model == "UMass-gbq_qr" ~ "GBQR",
      model == "UMass-gbq_qr_fit_locations_separately" ~ "GBQR-by-location",
      model == "UMass-gbq_qr_hhs_only" ~ "GBQR-only-NHSN",
      model == "FluSight-baseline" ~ "Baseline-flat"
    )
  )

joint_training_scores_table <- make_scores_table(
  joint_training_scores,
  caption = NULL,
  label = "junk label",
  bold_modelnames = FALSE
)
table_2b <- capture.output(
  print(joint_training_scores_table,
        include.rownames = FALSE, sanitize.text.function = function(x) {x})
)
table_2b <- c(
  "\\\\",
  "\\multicolumn{8}{l}{Experiment B: Reduced training data} \\\\",
  table_2b[6:13]
)

preprocessing_scores <- read_csv("../artifacts/scores/scores_by_model_flusion_data_adj_without_revisions.csv") |>
  mutate(
    Model = case_when(
      model == "UMass-gbq_qr" ~ "GBQR",
      model == "UMass-gbq_qr_no_reporting_adj" ~ "GBQR-no-reporting-adj",
      model == "UMass-gbq_qr_no_transform" ~ "GBQR-no-transform",
      model == "FluSight-baseline" ~ "Baseline-flat"
    )
  )

preprocessing_scores_table <- make_scores_table(
  preprocessing_scores,
  caption = paste("Evaluation results for post hoc experiments investigating determinants of model performance, omitting forecasts made on combinations of reference date and location for which the latest available NHSN data at the time of the forecast were subsequently revised by 10 or more admissions.  Experiment A gives results for individual component models in the Flusion ensemble, ensembles of pairs of components, and the full Flusion ensemble including all three components.  Experiment B gives results for the GBQR model, which is trained jointly on data for all locations and data sources, and variations trained separately for each location (GBQR-by-location) and trained only on hospital admissions from NHSN (GBQR-only-NHSN).  Experiment C gives results for a variation on the GBQR model that does not incorporate reporting adjustments designed to improve the degree to which ILINet and FluSurvNET data reflect influenza activity (GBQR-no-reporting-adj) and a variation that does not use a fourth-root transform (GBQR-no-transform), along with the original GBQR model which uses the reporting adjustments and the fourth-root transform. The percent of all combinations of location, reference date, and horizon for which the given model submitted forecasts is shown in the ``\\% Submitted\" column; in these retrospective experiments, we produced forecasts for all locations and time points. Within each experiment group, results for the model with the best MWIS, RWIS, MAE, and RAE are highlighted. Results for the models where empirical PI coverage rates are closest to the nominal levels are highlighted."),
  label = "tab:scores_experiments",
  bold_modelnames = FALSE
)
table_2c <- capture.output(
  print(preprocessing_scores_table,
        include.rownames = FALSE, sanitize.text.function = function(x) {x})
)

table_2c <- c(
  "\\\\",
  "\\multicolumn{8}{l}{Experiment C: Data preprocessing} \\\\",
  table_2c[6:17]
)

table_2 <- c(table_2a, table_2b, table_2c)
cat(table_2, sep = "\n")
@

\bibliography{flusion}

\end{document}
