\documentclass{article}

\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

\usepackage{amsmath, amsfonts, amssymb}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{hyperref}

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator{\short}{sh}
\DeclareMathOperator{\Ex}{\mathbb{E}}


\usepackage{setspace}
\onehalfspacing

\usepackage{parskip}

\usepackage{soul}
\usepackage{xcolor}
\def\elr#1{{\color{cyan}\textbf{ELR:[#1]}}}
\def\apg#1{{\color{red}\textbf{APG:[#1]}}}
\def\bwr#1{{\color{violet}\textbf{BWR:[#1]}}}
\def\ngr#1{{\color{blue}\textbf{NGR:[#1]}}}

\usepackage{natbib}
\bibliographystyle{unsrtnat-abbr}


\title{Forecasting influenza by training on multiple data sources}
\author{Evan L. Ray, possible additions include Nicholas G. Reich, Russ Wolfinger, Yijin Wang}

\begin{document}

\maketitle

<<setup, tidy=FALSE, echo=FALSE, message=FALSE>>=
knitr::opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE)
@

<<environment-setup>>=
library(readr)
library(dplyr)
library(xtable)

source("../code/utils.R")

## necessary to have project root as wd, to override document directory
# knitr::opts_knit$set(root.dir = '../')
@

\begin{abstract}


\end{abstract}

\section{Introduction}

There is a long history of forecasting seasonal influenza, often through formal collaborative forecasting exercises organized by the US Centers for Disease Control and Prevention (CDC).
\begin{itemize}
\item Long history going back 10 years
\item After a pause during the first two years of the COVID-19 pandemic, this forecasting exercise restarted in the 2022/23 flu season.
\end{itemize}

A challenge in recent forecasting exercises is that new data streams have come online which are being used as the ``ground truth'' target for these forecasting exercises.  These data sources have been great in many ways, but the lack of an extensive history of data for model training introduces difficulty for learning about seasonal patterns in flu.  To address this challenge, this season we developed a new model, called `flusion`, which pulls in data from external data sources with a long history of observations.  This model was the top-performing model that was contributed to the FluSight Forecast Hub in the 2023/24 influenza season.  The purpose of this paper is to describe that model and investigate what aspects of its design were associated with its strong performance.

\section{Data Sources}

Our model uses influenza data from four data sources, which are combined to produce three signals measuring the intensity of influenza activity over time and space:
\begin{itemize}
\item `NHSN`: Influenza hospital admissions as reported at the state level in a data set that is collected as part of the National Healthcare Safety Network (NHSN)
\item `ILI+`: An approximate measure of 
\item `FluSurv`: 
\end{itemize}


\section{Model}

Our model was constructed as an ensemble of statistical time series and machine learning models.  We begin describe the component models in sections \ref{subsec:model_sarix} and \ref{subsec:model_gbm} and the ensemble methods in section \ref{subsec:methods_ensemble}.  The precise formulation of the models included in the ensemble and the ensembling methods varied slightly over the course of the season, and we describe these aspects of our setup for generating real-time predictions in section \ref{subsec:model_realtime}.

\subsection{Component Model 1: SARIX}



\subsection{Component Model 2: GBM}



\subsection{Ensembling Methods: Quantile averaging and linear pools}



\subsection{Model adjustments used for real time forecasts}


\section{Evaluation metrics}

In the sections below, we evaluate forecasts using two metrics: the weighted interval score (WIS) and quantile coverage rates.

We also compute relative versions of these metrics.

\section{Real-time performance: the 2023/24 season}

In this section, we summarize model performance results for real-time submissions to the FluSight Forecast Hub in the 2023/24 season.

\subsection{Evaluation setup}

To avoid distortion of WIS and AE results, we did not evaluate forecasts that were made at the national level. Although the FluSight forecast hub originally allowed for collection of predictions at a ``horizon" of -1 week, these were discontinued; our analysis includes predictions made at horizons of 0 weeks (``nowcasts"), and predictions at horizons of 1, 2, and 3 weeks ahead relative to the reference date.

We included all models that contributed forecasts for at least 75\% of the combinations of state-level locations, reference dates, and non-negative horizons for which the Hub collected forecasts over the course of the season. Because a comprehensive evaluation of all Hub contributors is not the aim of this manuscript, we have anonymized the names of other individually-contributed models in these results to focus attention on the comparisons that are of interest for our purposes.

The FluSight hub produced two ensemble forecasts during the season: one using a quantile averaging approach and one using a linear pool. These two ensembles had similar performance, though the linear pool had slightly better calibration. However, the quantile averaging ensemble was used by CDC as the source of official communications throughout the season, and so we include results from only that ensemble here.

We also included results from two baseline methods. \textbf{Baseline-flat} is a random walk model produced by the Hub (labeled as \textbf{FluSight-baseline} in Hub submissions), which produces forecasts that extend from the most recent observation in a flat line, with expanding uncertainty based on historical differences in weekly hospital admissions. In this method, for each location $i$ the historical differences $\delta_{i,t} = y_{i,t} - y_{i,t-1}$ are collected, along with their negative values $-\delta_{i,t}$ (a process which we refer to as ``symmetrizing'' the differences). Forecasts at multiple step-ahead horizons are generated by iteratively sampling from this collection of symmetrized weekly differences.

The second baseline method, \textbf{Baseline-trend}, follows a similar process with a few modifications that are designed so that the resulting forecasts tend to follow the trend of recent observations. It is a quantile averaging ensemble of 16 variations on the baseline method. Most importantly, it incorporates variations that do not symmetrize the past differences, and rather than using all available history, it collects differences in a rolling window of the past few weeks. The 16 variations are obtained by using different options for the rolling window size, the temporal resolution of data used as an input (daily or weekly), a data transformation that is applied (no transformation or square root), and whether or not symmetrization is used. We emphasize that although this baseline is more methodologically involved than \textbf{Baseline-flat}, it produces an epidemiologically naive forecast that pushes forward a local estimate of the trend observed over the few most recent weeks. This model is named \textbf{UMass-trends\_ensemble} in real-time Hub submissions.

\subsection{Evaluation results}

TODO: finalize language here/check that statements made hold up at the end of the season.

Aggregating across all forecast dates and forecast horizons, the Flusion model had the best performance as measured by RWIS and RAE among all models that contributed to the FluSight Forecast Hub (Table \ref{tab:scores_flusight}). The Flusion model was consistently among the top-ranking models contributing to the forecast hub for individual forecast reference dates and the forecast horizons (Figure \ref{fig:scores_flusight} (b)).
 
Additionally, while its prediction interval coverage rates tended to be underconfident (i.e., prediction intervals were too wide on average), the 95\% PI coverage rates for Flusion came closest to the nominal coverage rate among all models (Table \ref{tab:scores_flusight}). An examination of one-sided quantile coverage rates confirms that Flusion was unique among models contributed to the Hub in that its coverage rates for high quantile levels were too high (indicating that the upper tail of the predictive distributions tended to fall above the eventually observed data more often than expected), while nearly all other models missed in the upper tails more often than expected (Figure \ref{fig:scores_flusight} (c)). Overall, the probabilistic calibration of the Flusion model was comparable to or better than that of other models contributed to the Hub, and it was superior to the calibration of the baseline and ensemble models.

\begin{enumerate}
\item Figure: show forecasts over the course of the season for our model and for the hub ensemble for selected locations.
\end{enumerate}

TODO: Supplemental analysis with similar results, omitting forecasts affected by data revisions?


\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{../artifacts/figures/forecasts_flusight.pdf}
    \caption{Influenza data and forecasts for selected models.}
    \label{fig:forecasts_flusight}
\end{figure}


<<overall-scores, results='asis'>>=
overall_scores_table <- read_csv("../artifacts/scores/scores_by_model_flusight_all.csv") |>
  add_model_anon(
    highlighted_models = c(
      "UMass-flusion" = "Flusion",
      "FluSight-ensemble" = "FluSight-ensemble",
      "FluSight-baseline" = "Baseline-flat",
      "UMass-trends_ensemble" = "Baseline-trend"),
    number_others = TRUE
  ) |>
  mutate(Model = model_anon,
         `\\% Submitted` = prop * 100,
         MWIS = wis,
         RWIS = wis_scaled_relative_skill,
         MAE = ae_median,
         RAE = ae_median_scaled_relative_skill,
         `50\\% PI Cov.` = interval_coverage_50,
         `95\\% PI Cov.` = interval_coverage_95,
         .keep = "none") |>
  arrange(RWIS) |>
  xtable::xtable(
    caption = paste("Overall evaluation results for forecasts submitted to the FluSight Forecast Hub. Model names other than \\textbf{Flusion}, \\textbf{FluSight-ensemble}, \\textbf{Baseline-flat}, and \\textbf{Baseline-trend} are anonymized. The percent of all combinations of location, reference date, and horizon for which the given model submitted forecasts is shown in the ``\\% Submitted\" column; only models submitting at least 75\\% of forecasts were included. Results for the model with the best MWIS, RWIS, MAE, and RAE are highlighted. Results for the models where empirical PI coverage rates are closest to the nominal levels are highlighted."),
    label = "tab:scores_flusight")

digits(overall_scores_table) <- c(0, 0, 1, 1, 3, 1, 3, 3, 3)

# print with specified entries in bold font
# adapted from https://gist.github.com/floybix/452201
bold_entries <- cbind(
  !grepl("Other Model", overall_scores_table$Model),
  rep(FALSE, nrow(overall_scores_table)),
  overall_scores_table$MWIS == min(overall_scores_table$MWIS),
  overall_scores_table$RWIS == min(overall_scores_table$RWIS),
  overall_scores_table$MAE == min(overall_scores_table$MAE),
  overall_scores_table$RAE == min(overall_scores_table$RAE),
  abs(overall_scores_table[["50\\% PI Cov."]] - 0.5) == min(abs(overall_scores_table[["50\\% PI Cov."]] - 0.5)),
  abs(overall_scores_table[["95\\% PI Cov."]] - 0.95) == min(abs(overall_scores_table[["95\\% PI Cov."]] - 0.95))
)

display <- display(overall_scores_table)
digits <- digits(overall_scores_table)
for (i in 1:ncol(overall_scores_table)) {
  if (is.numeric(overall_scores_table[,i])) {
    overall_scores_table[,i] <- formatC(
      overall_scores_table[,i],
      digits = digits[i+1],
      format = display[i+1])
    display(overall_scores_table)[i+1] <- "s"
  }

  ## embolden
  yes <- bold_entries[,i]
  overall_scores_table[yes,i] <- paste("\\textbf{", overall_scores_table[yes,i], "}", sep = "")
}

print(overall_scores_table,
      include.rownames = FALSE, sanitize.text.function = function(x) {x})
@

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{../artifacts/figures/scores_flusight.pdf}
    \caption{Influenza data and evaluation results. Panel (a): Target influenza hospital admissions data for the 2023/24 season, aggregated across all forecasted state-level geographic units. Panel (b): RWIS for models contributing to the FluSight Forecast Hub, stratified by forecast horizon (panels) and target date (horizontal axis). Lower relative WIS indicates better forecast performance. To focus on areas of interest, RWIS values greater than 2.5 are not displayed. Panel (c): One-sided quantile coverage differential, computed as empirical coverage rate minus nominal coverage rate. A well-calibrated model has a differential of 0, while a conservative method (with wide prediction intervals) has a negative differential at nominal coverage rates that are less than 0.5 and a positive differential at nominal coverage rates greater than 0.5. In panels (b) and (c), we highlight performance for the Flusion model, two baselines, and the FluSight ensemble; results for other models contributed to the hub are shown in light grey.}
    \label{fig:scores_flusight}
\end{figure}

\section{Post hoc model exploration}

In this section, we conduct some additional investigations designed to inform an understanding of why this model performed as well as it did.  Specifically, the questions motivating these analyses are:
\begin{enumerate}
\item are the individual GBM and SARIX models good, or did we get benefits from ensembling?
\item how helpful is training on other data streams?
\item this season was relatively early (only 1-2 weeks later than last season) and had peak incidence nearly identical to last seasonâ€™s peak.  how much did we benefit from that?  how much would performance degrade in larger/smaller/later seasons?
\item why does the model seem to respond so strongly to local level features and what can we do about it?
\end{enumerate}


\section*{Acknowledgements}

This work has been supported by the National Institutes of General Medical Sciences (R35GM119582) and the U.S. CDC(1U01IP001122). The content is solely the responsibility of the authors and does not necessarily represent the official views of NIGMS, the National Institutes of Health, or CDC.

\bibliography{allocation}

\end{document}
