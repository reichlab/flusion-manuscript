\documentclass{article}

\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

\usepackage{amsmath, amsfonts, amssymb}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{hyperref}

\usepackage{float}

\usepackage[inline]{enumitem}

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator{\short}{sh}
\DeclareMathOperator{\Ex}{\mathbb{E}}


\usepackage{setspace}
\onehalfspacing

\usepackage{parskip}

\usepackage{soul}
\usepackage{xcolor}
\def\elr#1{{\color{cyan}\textbf{ELR:[#1]}}}
\def\ngr#1{{\color{blue}\textbf{NGR:[#1]}}}

%\usepackage{natbib}
\usepackage{biblatex} %Imports biblatex package
\addbibresource{flusion.bib} %Import the bibliography file

\title{Flusion: Integrating multiple data sources for accurate influenza predictions}
\author{Evan L. Ray, possible additions include Nicholas G. Reich, Russ Wolfinger, Yijin Wang}

\begin{document}

\maketitle

<<setup, tidy=FALSE, echo=FALSE, message=FALSE>>=
knitr::opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE)
@

<<environment-setup>>=
library(readr)
library(dplyr)
library(xtable)

source("../code/utils.R")

## necessary to have project root as wd, to override document directory
# knitr::opts_knit$set(root.dir = '../')
@

\begin{abstract}
Over the last ten years, the US Centers for Disease Control and Prevention (CDC) has organized an annual influenza forecasting challenge with the motivation that accurate probabilistic forecasts could improve situational awareness and yield more effective public health actions.
Starting with the 2021/22 influenza season, the forecasting targets for this challenge have been based on hospital admissions reported in the CDCâ€™s National Healthcare Safety Network (NHSN) surveillance system.
Reporting of hospital admissions through NHSN began during the COVID pandemic, and as such NHSN has only a limited amount of historical data about influenza hospitalizations.
To produce forecasts in the presence of limited data for the target surveillance system, we augmented these data with two signals that have a longer historical record: 1) ILI+, which estimates the proportion of outpatient doctor visits where the patient has influenza; and 2) rates of laboratory-confirmed influenza hospitalizations at a set of sentinel healthcare facilities.
Our model, Flusion, is an ensemble model that combines a Bayesian autoregressive model with two machine learning models using gradient boosting for quantile regression based on different feature sets.
The autoregressive model was trained on only data for the target surveillance signal, NHSN admissions, while the gradient boosting models were trained on all three data signals; all three models were trained jointly on data for multiple locations.
In each week of the influenza season, the models produced quantiles of a predictive distribution for influenza hospital admissions in each state in the current week and the following three weeks; the ensemble prediction was computed by averaging these quantile predictions.
Flusion emerged as the top-performing model in the CDC's influenza prediction challenge for the 2023/24 season, and in this article we investigate the factors contributing to its success.
We find that the model's strong performance was primarily driven by the use of a gradient boosting model that was trained jointly on data from multiple surveillance signals and multiple locations.
These results indicate the value of sharing information across multiple locations and surveillance signals, especially when doing so enlargens the pool of available data.
\end{abstract}

\section{Introduction}
\label{sec:intro}

There is a long history of forecasting seasonal influenza, often through formal collaborative forecasting exercises organized by the US Centers for Disease Control and Prevention (CDC).
\begin{itemize}
\item Long history going back 10 years
\item After a pause during the first two years of the COVID-19 pandemic, this forecasting exercise restarted in the 2022/23 flu season.
\end{itemize}

A challenge in recent forecasting exercises is that new data streams have come online which are being used as the ``ground truth'' target for these forecasting exercises.  These data sources have been great in many ways, but the lack of an extensive history of data for model training introduces difficulty for learning about seasonal patterns in flu.  To address this challenge, this season we developed a new model, called `flusion`, which pulls in data from external data sources with a long history of observations.  This model was the top-performing model that was contributed to the FluSight Forecast Hub in the 2023/24 influenza season.  The purpose of this paper is to describe that model and investigate what aspects of its design were associated with its strong performance.

Relevant literature to review, ~1 paragraph each:
\begin{itemize}
\item methods for infectious disease forecasting, including methods that integrate multiple data sources.
    \begin{itemize}
    \item David Farrow cmu thesis on data fusion
    \item maybe papers using surveillance signals like Quidel?
    \item maybe some covid papers using things like mobility data
    \end{itemize}
\item forecasting with gradient boosting.
    \begin{itemize}
    \item Russ's paper
    \item M5 results paper
    \end{itemize}
\item transfer learning/data fusion
    \begin{itemize}
    \item will need to look into options
    \end{itemize}
\end{itemize}

\section{Data Sources}
\label{sec:data}

Our model uses three measures of influenza activity (Figure \ref{fig:data_overview}):
\begin{itemize}
\item NHSN: Influenza hospital admissions as reported at the state level in a data set that is collected as part of the National Healthcare Safety Network (NHSN).
\item FluSurv: Hospital admissions where the patient has a positive influenza test, expressed as a rate per 100,000 population in the catchment areas of specified hospital facilities as reported by FluSurv-NET.
\item ILI+: An approximate measure of the proportion of outpatient doctor visits where the patient has influenza.  This is derived by combining data from ILINet and WHO/NREVSS.
\end{itemize}

\begin{figure}[!h]
    \centering
    \includegraphics[width=\textwidth]{../artifacts/figures/data_overview.pdf}
    \caption{Influenza data at the national level in the US. The top panel shows weekly hospital admissions from NHSN, including the 2023/24 season that was the target season for predictions described in this article. In the second panel, a dashed orange line shows raw data reported from FluSurv-NET; the modeled data, in blue, are obtained by scaling up the raw data using per-season inflation factors designed to account for varing testing rates and test sensitivity/specificity.  In the third panel, a dashed orange line shows raw ILI data from ILINet; the modeled data in blue are ILI+ values obtained by combining ILI with test positivity rates. Grey shaded regions indicate pandemic seasons that were not used for model training; these include the 2008/09 and 2009/10 seasons which were impacted by pandemic swine influenza, and the 2020/21 and 2021/22 seasons which were impacted by low influenza activity during the COVID pandemic. Additionally, FluSurv-NET and ILI+ data for the 2023/24 season were not used for model training in this work.}
    \label{fig:data_overview}
\end{figure}

The FluSurv and ILI+ signals we used both included adjustments designed to correct for known challenges with interpreting the data collected by FluSurv-NET and ILINet.  The FluSurv-NET data report on patients with a positive influenza test, and as such are subject to varying levels of underreporting depending on changing testing rates and test sensitivity and specificity from season to season.  The CDC produces annual estimates of hospital burden due to influenza that adjust for these factors (TODO: cite).  We used these total burden estimates to estimate season-specific inflation factors that scale up reported rates from FluSurv-NET, with the intent of producing a more consistent measure of influenza activity over time; see supplemental section TODO for more details.  These inflation factors were generally larger in earlier seasons than in later seasons, indicating that FluSurv-NET undercounted influenza activity more in early seasons than it did in later seasons.

ILINet reports a measure of influenza-like illness (ILI), as the percent of outpatient doctor visits where the patient has symptoms consistent with influenza without another known cause. Because ILI is defined symptomatically, this signal generally includes some patients who have respiratory diseases other than influenza such as RSV and COVID. To address this, we compute ILI+ as the product of this ILI signal and influenza test positivity rates as reported by laboratory testing sites reporting to World Health Organization (WHO) and National Respiratory Enteric Virus Surveillance System (NREVSS) systems.  After converting to a proportion scale, ILI+ can be interpreted as an estimate of the proportion of outpatient doctor visits where the patient has influenza, and has been used in previous forecasting work as a more specific measure of influenza activity than ILI \cite[e.g.][]{goldstein2011-predicting-epidemic-sizes-flu-strains, shaman2013-influenza-forecast-2012-2013}.

Our models are fit to preprocessed versions of the surveillance data, with transformations designed to out the data on a similar scale for different surveillance signals and across different locations. Denote the value of a particular surveillance signal $s$ in location $l$ at time $t$ by $z_{l, s, t}$. We apply the following operations to compute the transformed version of the signal, $\tilde{z}_{l, w, t}$:
\begin{enumerate}
\item For NHSN admissions, we divide by the population of the location $l$ in units of 100,000 people to convert to a hospital admissions rate per 100,000 population, which is comparable across locations of different sizes. Note that the ILI+ and FluSurv signals are naturally expressed as rates or percents and so the magnitude of those signal does not depend on population size.
\item We take a fourth root transformation to stabilize the variance of the signal across times of low and high influenza activity.
\item We scale by dividing by the 95th percentile of all observations for each location and data source, and center by subtracting the mean for each location and data source.  These transformations adjust for varying magnitudes of the surveillance signals for different data sources and locations.
\end{enumerate}
The resulting transformed data used as an input to the model are shown in Figure \ref{fig:data_standardized}.

\begin{figure}[!h]
    \centering
    \includegraphics[width=\textwidth]{../artifacts/figures/data_standardized.pdf}
    \caption{Influenza data for all surveillance signals and all locations available for each data source after standardizing transformations have been applied. The top panel shows weekly hospital admissions from NHSN, the second panel shows data from FluSurv-NET, and the third panel shows ILI+. Within each panel, there is one line for each combination of season and location for all seasons and locations that are available for the given surveillance system at the state, regional, and national levels.  Line color corresponds to the population size of the location; the darkest lines are for the national level while the lightest lines are for states with small populations.}
    \label{fig:data_standardized}
\end{figure}

\section{Model}
\label{sec:model}

Our model was constructed as an ensemble of statistical time series and machine learning models.  We describe the component models in sections \ref{subsec:model_sarix} and \ref{subsec:model_gbm} and the ensemble methods in section \ref{subsec:methods_ensemble}.  The precise formulation of the models included in the ensemble and the ensembling methods varied slightly over the course of the season, and we describe these aspects of our setup for generating real-time predictions in section \ref{subsec:model_realtime}.

\subsection{Component Models 1 and 2: GBM}

The GBM models learn a mapping from features $x_i$ to prediction target $y_i$, where $i$ indexes a combination of location $l(i)$, data source $s(i)$, reference date $d(i)$, and forecast horizon $h(i)$.  Specifically, we fit 23 quantile regression models, one for each required quantile level $\alpha_k, k = 1, \ldots, 23$, using the pinball loss as the learning objective.  Models are fit using the LightGBM package in Python with default settings for all hyperparameters.  The final prediction at each quantile level is obtained using bagging, by taking the median of predictions from 100 separate fits, where each fit is based on a randomly selected 70\% of the seasons in the training set (including partial data for the current season).

The models are trained simultaneously for all locations, data sources, and forecast horizons.  However, the features $x_i$ contain information only about influenza activity for the particular location $l(i)$ and data source $s(i)$. We emphasize that inclusion of multiple locations and data sources in the training data set allows the model to use past examples from multiple locations and data sources to learn a mapping from $x$ to $y$, but in our model setup, predictions of NHSN admissions in a particular location are not informed by contemporaneous observations of NHSN admissions in other locations or by contemporaneous observations of other surveillance signals in that same location. The use of contemporaneous observations from other locations or signals to inform predictions remains a topic for future work.

Both the features $x_i$ and the prediction targets $y_i$ are calculated based on a transformed version of the original surveillance signal. 
The prediction target $y_i$ is the difference between the transformed signal value on the target date, $d(i) + h(i)$, and the date of the last available reported data. A prediction of this target is converted to a prediction on the original scale by adding the last observed value and inverting the initial data transformation operations described above.

The primary GBM model is fit to the following 114 features:
\begin{enumerate}
\item A one-hot encoding of the data source.
\item A one-hot encoding of the location.
\item A one-hot encoding of the spatial scale of the location ("state", "region", or "national").
\item The population of the location.
\item The difference between the week of the season with the most recent reported data and Christmas week; for instance, a value of 3 means that the most recent data report are for the week three weeks after Christmas.
\item The coefficients of a degree 2 Taylor polynomial fit to the trailing $w$ weeks of data, where $w \in \{4, 6\}$, with the reference point for the polynomial set to the time $d(i)$.  These coefficients are estimates of the local level, first derivative, and second derivative of the signal at the time $d(i)$.
\item The coefficients of a degree 1 Taylor polynomial fit to the trailing $w$ weeks of data, where $w \in \{3, 5\}$. These coefficients are estimates of the local level and first derivative of the signal at the time $d(i)$.
\item The rolling mean of the signal over the last $w$ weeks, where $w \in \{2, 4\}$.
\item The values all features from points 6, 7, and 8 at lags 1 and 2, representing estimates of the local level and first and second derivatives of the signal in each of the previous two weeks.
\item The forecast horizon.
\end{enumerate}
TODO: plot of local deriv. features

Measures of the local level of the surveillance signal (i.e., rolling means and the intercepts of Taylor polynomial fits) had a high feature importance in the primary GBM model; see section \ref{sec:post-hoc-preprocessing} below for more detail.  Starting the week of TODO, we fit a second variation on the GBM model that was not allowed to see these ``local level" features. This was motivated by two considerations: (1) a model fit without features that had high importance in the primary GBM model might introduce more model diversity to the flusion ensemble; and (2) in seasons with particularly high or low incidence, measures of local level might not be a reliable indicator of the magnitude and direction of changes in future values.

\begin{itemize}
\item supplemental figure showing some feature values from roll mean, taylor coefs.  Figures similar to what's shown at \url{https://github.com/reichlab/timeseriesutils/blob/main/docs/demo.ipynb}
\end{itemize}

\subsection{Component Model 3: ARX}

We have implemented An auto-regressive time series model with external covariates (ARX). Development of this model is in progress, and we have not carefully evaluated many of the modeling decisions that have been made for this component. Here we describe the state of the model as it was used for real-time forecasting during the 2023/34 influenza season; we are actively pursuing refinements to this model setup, but a description of those extensions is deferred to future work.

The ARX model is trained jointly on data for all locations and all influenza signals (NHSN, ILI+, and FluSurv) [TODO double-check this -- i thought i fit to all signals, but i see a comment saying i did not]. However, as with the GBM models, the ILI+ and FluSurv signals are not used as real-time covariates measuring trends in influenza prevalence in the current season to inform NHSN predictions. Rather, those signals are used to provide additional training data to help inform estimation of parameters describing influenza dynamics. Our single covariate is a spike function indicating proximity to the week of Christmas, taking the value 

It is worth highlighting a few ways in which our ARX model differs from standard implementations. In our setup, the autoregressive coefficients are shared across locations and datasets, while each location is assigned a separate variance parameter for innovations. Additionally, unlike many ARX model specifications, our model does not take future values of the covariates as known, but rather it predicts those covariates alongside the primary prediction target. In our current modeling setting, this behavior is not ideal; remedying this to allow for the provision of known future values of covariates is on a short list of model improvements to make.

\begin{enumerate}
\item We take a Bayesian approach with a shrinkage prior on the AR coefficients.
\end{enumerate}

TODO write up model specification

\subsection{Ensembling Methods: Quantile averaging and linear pools}

TODO, we mostly did quantile averaging.  sometimes i used a linear pool when i didn't trust the latest data report.

\subsection{Model adjustments used for real time forecasts}

TODO, we tried a few different model variations over the course of the season.  To pull from \url{https://github.com/cdcepi/FluSight-forecast-hub/blob/main/model-metadata/UMass-flusion.yml}

\section{Evaluation metrics}
\label{sec:eval-metrics}

In the sections below, we evaluate forecasts using three metrics: the mean absolute error (MAE) of the predictive median, the weighted interval score (WIS), and one-sided quantile coverage rates. MAE measures the average distance between the predictive median and the eventual observation. WIS can be viewed as a generalization of the absolute error to a set of quantile predictions, and is equivalent to an average of quantile scores (sometimes referred to as pinball losses) computed for each quantile prediction. The quantile score for a single quantile prediction assigns an asymmetric penalty to the distance between the prediction and the observation, with the magnitudes of the penalties for underprediction and overprediction set so that in expectation, the quantile score is minimized by the quantile of the predictive distribution at a specified quantile level. The quantile score for the quantile level 0.5 is equivalent to the absolute error.

We also compute relative versions of MAE and WIS.

\section{Real-time performance: the 2023/24 season}
\label{sec:real-time-results}

In this section, we summarize model performance results for real-time submissions to the FluSight Forecast Hub in the 2023/24 season.

\subsection{Evaluation setup}

To avoid distortion of WIS and AE results, we did not evaluate forecasts that were made at the national level. Although the FluSight forecast hub originally allowed for collection of predictions at a ``horizon" of -1 week, these were discontinued; our analysis includes predictions made at horizons of 0 weeks (``nowcasts"), and predictions at horizons of 1, 2, and 3 weeks ahead relative to the reference date.

We included all models that contributed forecasts for at least 75\% of the combinations of state-level locations, reference dates, and non-negative horizons for which the Hub collected forecasts over the course of the season. Because a comprehensive evaluation of all Hub contributors is not the aim of this manuscript, we have anonymized the names of other individually-contributed models in these results to focus attention on the comparisons that are of interest for our purposes.

The FluSight hub produced two ensemble forecasts during the season: one using a quantile averaging approach and one using a linear pool. These two ensembles had similar performance, though the linear pool had slightly better calibration. However, the quantile averaging ensemble was used by CDC as the source of official communications throughout the season, and so we include results from only that ensemble here.

We also included results from two baseline methods. \textbf{Baseline-flat} is a random walk model produced by the Hub (labeled as \textbf{FluSight-baseline} in Hub submissions), which produces forecasts that extend from the most recent observation in a flat line, with expanding uncertainty based on historical differences in weekly hospital admissions. In this method, for each location $i$ the historical differences $\delta_{i,t} = y_{i,t} - y_{i,t-1}$ are collected, along with their negative values $-\delta_{i,t}$ (a process which we refer to as ``symmetrizing'' the differences). Forecasts at multiple step-ahead horizons are generated by iteratively sampling from this collection of symmetrized weekly differences.

The second baseline method, \textbf{Baseline-trend}, follows a similar process with a few modifications that are designed so that the resulting forecasts tend to follow the trend of recent observations. It is a quantile averaging ensemble of 16 variations on the baseline method. Most importantly, it incorporates variations that do not symmetrize the past differences, and rather than using all available history, it collects differences in a rolling window of the past few weeks. The 16 variations are obtained by using different options for the rolling window size, the temporal resolution of data used as an input (daily or weekly), a data transformation that is applied (no transformation or square root), and whether or not symmetrization is used. We emphasize that although this baseline is more methodologically involved than \textbf{Baseline-flat}, it produces an epidemiologically naive forecast that pushes forward a local estimate of the trend observed over the few most recent weeks. This model is named \textbf{UMass-trends\_ensemble} in real-time Hub submissions.

\subsection{Evaluation results}

Forecasts from the Flusion model often appeared similar to forecasts from the FluSight-ensemble, though in several states (e.g. Florida, California, and New York) Flusion did a better job of capturing the increase in weekly hospital admissions in the early part of the season and a slightly better job of predicting the turnaround after the peak in late December (Figure \ref{fig:forecasts_flusight}). Qualitatively, both the FluSight-ensemble and the Flusion model generally captured trends in hospital admissions better than the baseline models during all phases of the season -- during the rise in the early part of the season, near the peak, and on the way down. An exception to this can be seen in forecasts for New York, Pennsylvania, and Michigan that were produced with a reference date of February 3, 2024. Those states had two local peaks: one near Christmas, and a second in late February or early March. This was a common pattern in many states in the northern US in the 2023/24 season, and in these instances the Flusion model (as well as the FluSight ensemble) typically produced incorrect predictions of continued decreases after the first peak heading into the second peak.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{../artifacts/figures/forecasts_flusight.pdf}
    \caption{Influenza data and forecasts for the six states with the largest cumulative hospital admissions during the 2023/24 season. To avoid overplotting, in this figure forecasts from every fourth reference date are shown; evaluations include all reference dates. Forecasts are represented by the predictive median (black lines) and 50\% and 95\% prediction intervals (blue shaded regions). Solid orange lines show the finalized admission counts reported as of May 17, 2024, while dotted orange lines show the initial reported values that were available on the date predictions were generated.}
    \label{fig:forecasts_flusight}
\end{figure}

Aggregating across all forecast dates and forecast horizons, the Flusion model had the best performance as measured by RWIS and RAE among all models that contributed to the FluSight Forecast Hub (Table \ref{tab:scores_flusight}). The Flusion model was consistently among the top-ranking models contributing to the forecast hub for individual forecast reference dates and forecast horizons (Figure \ref{fig:scores_flusight} (b)).
 
Prediction intervals from the Flusion model tended to be underconfident, i.e., prediction intervals were too wide on average (Table \ref{tab:scores_flusight}). An examination of one-sided quantile coverage rates indicates that marginally, the predictive quantiles in the upper tail of the forecast distribution are fairly well calibrated, while predictions for the lower quantile levels were too small on average (Figure \ref{fig:scores_flusight} (c)). Overall, the probabilistic calibration of the Flusion model was comparable to or better than that of other models contributed to the Hub, and it was superior to the calibration of the baseline and ensemble models.

TODO: Supplemental analysis with similar results, omitting forecasts affected by data revisions.  Rankings don't change much.



<<overall-scores, results='asis'>>=
overall_scores <- read_csv("../artifacts/scores/scores_by_model_flusight_all.csv") |>
  add_model_anon(
    highlighted_models = c(
      "UMass-flusion" = "Flusion",
      "FluSight-ensemble" = "FluSight-ensemble",
      "FluSight-baseline" = "Baseline-flat",
      "UMass-trends_ensemble" = "Baseline-trend"),
    number_others = TRUE
  ) |>
  mutate(Model = model_anon)

make_scores_table <- function(scores, caption, label, bold_modelnames = TRUE) {
  scores_table <- scores |>
    mutate(Model = Model,
           `\\% Submitted` = prop * 100,
           MWIS = wis,
           RWIS = wis_scaled_relative_skill,
           MAE = ae_median,
           RAE = ae_median_scaled_relative_skill,
           `50\\% Cov.` = interval_coverage_50,
           `95\\% Cov.` = interval_coverage_95,
           .keep = "none") |>
    arrange(RWIS) |>
    xtable::xtable(
      caption = caption,
      label = label)

  digits(scores_table) <- c(0, 0, 1, 1, 3, 1, 3, 3, 3)

  # print with specified entries in bold font
  # adapted from https://gist.github.com/floybix/452201
  if (bold_modelnames) {
    bold_modelnames <- !grepl("Other Model", scores_table$Model)
  } else {
    bold_modelnames <- rep(FALSE, nrow(scores_table))
  }
  bold_entries <- cbind(
    bold_modelnames,
    rep(FALSE, nrow(scores_table)),
    scores_table$MWIS == min(scores_table$MWIS),
    scores_table$RWIS == min(scores_table$RWIS),
    scores_table$MAE == min(scores_table$MAE),
    scores_table$RAE == min(scores_table$RAE),
    abs(scores_table[["50\\% Cov."]] - 0.5) == min(abs(scores_table[["50\\% Cov."]] - 0.5)),
    abs(scores_table[["95\\% Cov."]] - 0.95) == min(abs(scores_table[["95\\% Cov."]] - 0.95))
  )

  display <- display(scores_table)
  digits <- digits(scores_table)
  for (i in 1:ncol(scores_table)) {
    if (is.numeric(scores_table[,i])) {
      scores_table[,i] <- formatC(
        scores_table[,i],
        digits = digits[i+1],
        format = display[i+1])
      display(scores_table)[i+1] <- "s"
    }

    ## embolden
    yes <- bold_entries[,i]
    scores_table[yes,i] <- paste("\\textbf{", scores_table[yes,i], "}", sep = "")
  }
  
  return(scores_table)
}

overall_scores_table <- make_scores_table(
  overall_scores,
  caption = paste("Overall evaluation results for forecasts submitted to the FluSight Forecast Hub. Model names other than \\textbf{Flusion}, \\textbf{FluSight-ensemble}, \\textbf{Baseline-flat}, and \\textbf{Baseline-trend} are anonymized. The percent of all combinations of location, reference date, and horizon for which the given model submitted forecasts is shown in the ``\\% Submitted\" column; only models submitting at least 2/3 of forecasts were included. Results for the model with the best MWIS, RWIS, MAE, and RAE are highlighted. Results for the models where empirical PI coverage rates are closest to the nominal levels are highlighted."),
  label = "tab:scores_flusight"
)

print(overall_scores_table,
      include.rownames = FALSE, sanitize.text.function = function(x) {x})
@

\begin{figure}[!h]
    \centering
    \includegraphics[width=\textwidth]{../artifacts/figures/scores_flusight.pdf}
    \caption{Influenza data and evaluation results. Panel (a): Weekly influenza hospital admissions reported in NHSN for the 2023/24 season, aggregated across all forecasted state-level locations. Panel (b): RWIS for models contributing to the FluSight hub, by forecast horizon (panels) and target date (horizontal axis). Lower relative WIS indicates better forecast performance. RWIS values greater than 2.5 are not displayed. Panel (c): One-sided quantile coverage differential, computed as empirical coverage rate minus nominal coverage rate. A well-calibrated model has a differential of 0, while a conservative method (with wide prediction intervals) has a negative differential at nominal coverage rates less than 0.5 and a positive differential at nominal coverage rates greater than 0.5.}
    \label{fig:scores_flusight}
\end{figure}

\section{Post hoc model exploration}
\label{sec:post-hoc-results}

In this section, we investigate the degree to which the following aspects of the Flusion model contibuted to its strong performance:
\begin{enumerate*}
\item the formulation of Flusion as an ensemble of three individual models;
\item joint training on multiple data sets and multiple locations;
\item data preprocessing, including corrections for reporting inconsistencies in the ILINet and FluSurvNET data, the use of a fourth root data transform, and the importance of the features that were used by the GBQ model.
\end{enumerate*}

\subsection{Component models and ensembling}

To investigate the skill of our individual component models and the added value of ensembling, we computed scores for each of the three component models that were members of the Flusion ensemble and for ensembles formed using two of the three components.  As documented in section \ref{sec:model}, the component models and ensembling method that we used in real time changed over the course of the season. To enable a clearer understanding of the contributions of these models, the results we present here are based on the specifications of the individual GBQ, GBQ-no-level, and ARX models and the quantile averaging ensemble method that were used for the Flusion model starting the week of December 2, 2023.  In instances where predictions from one or more component models were not created in real time, we created post hoc model fits and predictions using the data that would have been available in real time.

<<posthoc-scores, results='asis'>>=
component_scores <- read_csv("../artifacts/scores/scores_by_model_flusion_components.csv") |>
  mutate(
    Model = case_when(
      model == "UMass-flusion__gbq_qr__sarix" ~ "GBQ, ARX",
      model == "UMass-flusion" ~ "Flusion",
      model == "UMass-gbq_qr" ~ "GBQ",
      model == "UMass-flusion__gbq_qr__gbq_qr_no_level" ~ "GBQ, GBQ-no-level",
      model == "UMass-flusion__gbq_qr_no_level__sarix" ~ "GBQ-no-level, ARX",
      model == "UMass-gbq_qr_no_level" ~ "GBQ-no-level",
      model == "UMass-sarix" ~ "ARX",
      model == "FluSight-baseline" ~ "Baseline-flat"
    )
  )

component_scores_table <- make_scores_table(
  component_scores,
  caption = NULL,
  label = "junk label",
  bold_modelnames = FALSE
)
table_2a <- capture.output(
  print(component_scores_table,
        include.rownames = FALSE, sanitize.text.function = function(x) {x})
)
table_2a <- c(
  table_2a[1:5],
  "\\multicolumn{8}{l}{Experiment A: Component model performance} \\\\",
  table_2a[6:17]
)

joint_training_scores <- read_csv("../artifacts/scores/scores_by_model_joint_training.csv") |>
  mutate(
    Model = case_when(
      model == "UMass-gbq_qr" ~ "GBQ",
      model == "UMass-gbq_qr_fit_locations_separately" ~ "GBQ-by-location",
      model == "UMass-gbq_qr_hhs_only" ~ "GBQ-only-NHSN",
      model == "FluSight-baseline" ~ "Baseline-flat"
    )
  )

joint_training_scores_table <- make_scores_table(
  joint_training_scores,
  caption = NULL,
  label = "junk label",
  bold_modelnames = FALSE
)
table_2b <- capture.output(
  print(joint_training_scores_table,
        include.rownames = FALSE, sanitize.text.function = function(x) {x})
)
table_2b <- c(
  "\\\\",
  "\\multicolumn{8}{l}{Experiment B: Reduced training data} \\\\",
  table_2b[6:13]
)

preprocessing_scores <- read_csv("../artifacts/scores/scores_by_model_flusion_data_adj.csv") |>
  mutate(
    Model = case_when(
      model == "UMass-gbq_qr" ~ "GBQ",
      model == "UMass-gbq_qr_no_reporting_adj" ~ "GBQ-no-reporting-adj",
      model == "UMass-gbq_qr_no_transform" ~ "GBQ-no-transform",
      model == "FluSight-baseline" ~ "Baseline-flat"
    )
  )

preprocessing_scores_table <- make_scores_table(
  preprocessing_scores,
  caption = paste("Evaluation results for post hoc experiments investigating determinants of model performance.  Experiment A gives results for individual component models in the Flusion ensemble, ensembles of pairs of components, and the full Flusion ensemble including all three components.  Experiment B gives results for the GBQ model, which is trained jointly on data for all locations and data sources, and variations trained separately for each location (GBQ-by-location) and trained only on hospital admissions from NHSN (GBQ-only-NHSN).  Experiment C gives results for a variation on the GBQ model that does not incorporate reporting adjustments designed to improve the degree to which ILINet and FluSurvNET data reflect influenza activity (GBQ-no-reporting-adj) and a variation that does not use a fourth-root transform (GBQ-no-transform), along with the original GBQ model which uses the reporting adjustments and the fourth-root transform. The percent of all combinations of location, reference date, and horizon for which the given model submitted forecasts is shown in the ``\\% Submitted\" column; in these retrospective experiments, we produced forecasts for all locations and time points. Within each experiment group, results for the model with the best MWIS, RWIS, MAE, and RAE are highlighted. Results for the models where empirical PI coverage rates are closest to the nominal levels are highlighted."),
  label = "tab:scores_experiments",
  bold_modelnames = FALSE
)
table_2c <- capture.output(
  print(preprocessing_scores_table,
        include.rownames = FALSE, sanitize.text.function = function(x) {x})
)

table_2c <- c(
  "\\\\",
  "\\multicolumn{8}{l}{Experiment C: Data preprocessing} \\\\",
  table_2c[6:17]
)

table_2 <- c(table_2a, table_2b, table_2c)
cat(table_2, sep = "\n")
@

Although two of the ensembles outperformed the GBQ model alone, the most important determinant of performance was whether or or not the GBQ model was included (Table \ref{tab:scores_experiments}, Experiment A). Score differences among the top four model variations were small, and all of those model variations included the GBQ model. There was a drop-off in performance for other variations that did not include GBQ. Thus, the fact that Flusion was constructed as an ensemble of three models was not a key driver of its performance. Indeed, GBQ alone and GBQ-no-level alone would each have placed first among all FluSight submissions, while ARX would have placed third among all contributing models.

The GBQ-no-level model, which was trained without access to features measuring the local level of the time series, had worse performance than the primary GBQ model, and ensembles that included it generally performed slightly worse than ensembles that did not include it. We view this as evidence that the approach of omitting local level features was harmful to individual model performance without introducing enough differentiation from the primary GBQ model to serve as a useful ensemble member. In contrast, although ARX was the worst individual model, ensemble variations that included ARX were slightly better than ensemble variations that did not include it. Including models with more structural differences can be helpful in an ensemble, although in these results the gains in performance from including ARX were generally small.

\subsection{Joint training on multiple data sets and multiple locations}

The primary GBQ model was trained jointly on data from all three surveillance signals (NHSN, ILINet, and FluSurv) and on data for all locations at the state, HHS regional, and US national level. To investigate the value of this joint model training approach, we considered two alternative methods:
\begin{enumerate}
\item The GBQ-only-NHSN model was trained on data for all locations, but using only hospital admissions from NHSN, the surveillance signal used as the prediction target.
\item The GBQ-by-location model was trained separately for each state-level jurisdiction using data for that location from all three data sources.
\end{enumerate}

Both of these alternatives underperformed relative to the GBQ model (Table \ref{tab:scores_experiments}, Experiment B). GBQ-by-location would have been among the top three contributing models to the forecast hub, and GBQ-only-NHSN would have been among the top ten contributing models; however, both would have underperformed relative to the FluSight ensemble. The decisions to train on multiple data sources and to train jointly on data for all locations were critical for achieving strong model performance.

\subsection{Data preprocessing}
\label{sec:post-hoc-preprocessing}

In a third experiment, we fit two model variations to investigate the value of some of the data preprocessing steps we used.  The GBQ-no-reporting-adj model omitted the adjustments described in Section \ref{sec:data} that were intended to address reporting inconsistencies in the ILINet and FluSurvNET data. Specifically, this model used the ILI signal directly rather than using test positivity rates to convert to ILI+, and it used the raw rates reported by FluSurvNET rather than attempting to account for time-varying case capture rates in the FluSurvNET system. In our evaluations, GBQ-no-reporting-adj outperformed the original GBQ model by a small amount (Table \ref{tab:scores_experiments}, Experiment C). These reporting adjustments were not helpful to model performance, and indeed the evidence suggests that they were counterproductive.

In a second model variation, we investigated whether or not the use of a fourth root data transform was helpful. The GBQ-no-transform model was fit to data without using a power transform, though other transformations described in section \ref{sec:data} were used, including converting hospital admission to a rate per 100,000 population and applying centering and scaling operations to make the data more comparable across different locations and data sources. The GBQ-no-transform model had slightly worse performance than the original GBQ model, indicating that the power transform was helpful (Table \ref{tab:scores_experiments}, Experiment C).

We also investigated feature importance as measured by the number of times each feature was used for the splitting criterion in a tree node in the gradient boosting fits for the GBQ model (Supplemental Figure TODO). For this investigation, we used a representative fit from the reference date of January 6, 2024. We averaged the importance score across the gradient boosting fits from all 100 bags and all 23 quantile levels.  The top five features were the current season week, the population of the target location, the most recent observation of the surveillance signal (after preprocessing transformations), the forecast horizon, and the difference between the current season week and Christmas week.  These were followed by a group of features that also had fairly high importance, primarily consisting of features measuring the local level, trend, and curvature of disease incidence, as well as an indicators of whether the location was Puerto Rico (which sees substantively different trends in influenza activity than other locations), and indicators of what the data source was. A final group of features with lower importance included indicators for all other locations and indicators of the aggregation level for the location (state, regional, or national).

\section{Discussion}

summary
\begin{itemize}
\item we won
\item thing that mattered most was training jointly on data from multiple surveillance signals and locations
\item other decisions had a more minor impact or were unhelpful: data transformations, attempting to correct for reporting artifacts, adding in ensemble members other than our main GBQR model.
\end{itemize}

next steps
\begin{itemize}
\item incorporating spatial dependence
\item contemporaneous use of multiple signals
\item other possible signals -- quidel, insurance claims
\item vaccine uptake and efficacy
\item better handling of holiday effects and the possibility of a second peak
\item flu strain data -- how to use, laggy?
\item improvements to ARX model
\end{itemize}

\section*{Acknowledgements}

This work has been supported by the National Institutes of General Medical Sciences (R35GM119582) and the U.S. CDC(1U01IP001122). The content is solely the responsibility of the authors and does not necessarily represent the official views of NIGMS, the National Institutes of Health, or CDC.

\printbibliography

\end{document}
